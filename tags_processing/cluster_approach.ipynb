{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load latitude and longitude\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.cluster.vq import kmeans2, whiten\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\scipy\\cluster\\vq.py:653: UserWarning: One of the clusters is empty. Re-run kmean with a different initialization.\n",
      "  warnings.warn(\"One of the clusters is empty. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from geopy.distance import great_circle\n",
    "#from shapely.geometry import MultiPoint\n",
    "\n",
    "\n",
    "X=df.loc[:,['latitude','longitude']]\n",
    "\n",
    "x, y = kmeans2(X, 500, iter = 20)  #<--- I randomly picked 500 clusters\n",
    "plt.scatter( X.loc[:,'longitude'], X.loc[:,'latitude'], c=y, alpha=0.33333);\n",
    "\n",
    "# so we have centroids, there are 500 of them\n",
    "# i will save them into csv file\n",
    "\n",
    "df = pd.DataFrame(x)\n",
    "df.to_csv(\"500_clusters.csv\", encoding=\"utf-8\",index=True, header = ['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I will assign for each image label from each cluster\n",
    "\n",
    "df = pd.read_csv('dm_clean1.csv')\n",
    "df_clusters = pd.read_csv(\"500_clusters.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "X = df_clusters.as_matrix(columns=['latitude', 'longitude'])\n",
    "nbrs = NearestNeighbors(n_neighbors=1,algorithm='ball_tree').fit(X)#<- KNN\n",
    "\n",
    "X1 = df.as_matrix(columns=['latitude', 'longitude'])\n",
    "distances , indices = nbrs.kneighbors(X1)\n",
    "\n",
    "#indices = pd.DataFrame({'clusters':indices})\n",
    "df['clusters'] = indices\n",
    "\n",
    "df.to_csv(\"data_500clusters.csv\", encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_500clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate all the texts to the appropriate cluster\n",
    "\n",
    "df = df.drop(['latitude','longitude','photoID', 'userID', 'photoLink', 'DateTaken'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to be applied that takes a row and a list of columns \n",
    "# to concatenate\n",
    "def concat_text(row, cols):\n",
    "    # The real work is done here\n",
    "    return \" \".join([\" \".join([str(x) for x in y if x]) for y in row[cols].values])\n",
    "\n",
    "result = df.groupby('clusters').apply(concat_text, ['photoTags']) # groupby and apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv(\"500clusters_with_tags.csv\", encoding=\"utf-8\", header=['photoTags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     clusters                                          photoTags\n",
      "0           0  idaho backpacking sawtoothmountains sawtoothwi...\n",
      "1           1  anais park ramenskoe park ramenskoe road  coun...\n",
      "2           4  building architecture vintage 2006 railwaystat...\n",
      "3           5           thenationalarchivesuk africathroughalens\n",
      "4           6  family india me kolkata calcutta middleschool ...\n",
      "5           7  sunset sun beach washington sand graysharbor o...\n",
      "6           8  city montana downtown mt miles foe eaglesclub ...\n",
      "7          10  blackandwhite bw white building church archite...\n",
      "8          12  voyage travel church  iceland glise canonef247...\n",
      "9          13                             pakistan karachi sindh\n",
      "10         15  autumn canada sign saskatoon saskatchewan neig...\n",
      "11         16  khajuraho khajuraho madhyapradesh people peopl...\n",
      "12         18  starlake goldriver vancouverislandfallnaturepa...\n",
      "13         19  egypt egitto egypt egitto egypt egitto abusimb...\n",
      "14         20  muffin hotwater controls waterheater illinois ...\n",
      "15         23  hipstamatic kodotverichromefilm kaimalmarkiile...\n",
      "16         24  orange bird animals zoo sandiego evil colourfu...\n",
      "17         25  sahara desert morocco maroc desierto marruecos...\n",
      "18         26  africa vintage north slide scan nigeria busa b...\n",
      "19         27  africa holiday truck bedford nigeria 1995 kano...\n",
      "20         28           sevilla conference ebe07 dopplrtrip42931\n",
      "21         29  china motorcycle chongqing sichuan panning bei...\n",
      "22         30  365days 365days 365days 365days world station ...\n",
      "23         31  tenerife gomera lagomera elhierro viajes lapal...\n",
      "24         32  france inflight video air north jet cockpit at...\n",
      "25         35  italy italia sicily 2008 sicilia aeolianisland...\n",
      "26         37   venezuela canaima roraima tepui geotoolyuancc...\n",
      "27         38  gourds longisland 2007 faved october2007 longi...\n",
      "28         40   holidays crete geotoolgmif geolat34944418 geo...\n",
      "29         41  voyage travel nature  iceland seals phoque isl...\n",
      "..        ...                                                ...\n",
      "376       463  tree message lithuania vilnius tump fire firee...\n",
      "377       466  lake titicaca bolivia peru laketiticaca puno p...\n",
      "378       467                shozu  geolon12964270 geolat2462883\n",
      "379       468                                      100 100 india\n",
      "380       470  madeira gagilas stilllife sepia  wine barrels ...\n",
      "381       471  hawaii aerial kauai hawaiianislands kauaitrip2...\n",
      "382       472  blue palenque suramerica managua nicaraguagent...\n",
      "383       473  aquarium jellyfish fortfisher aquarium loggerh...\n",
      "384       474  house la louisiana rita hurrican raised delcam...\n",
      "385       476  stone desert cave saudiarabia alhasa rickredds...\n",
      "386       477  suer lake water sculpture face pumpkin october...\n",
      "387       478  africa holiday history islam explore arab liby...\n",
      "388       479  scotland clyde steamer firth tighnabruaich car...\n",
      "389       480  freedom 1st aviation iraq attack edward anthon...\n",
      "390       481  china beijing architecture russia towers mason...\n",
      "391       482  tennessee union hobby civilwarveteran tombston...\n",
      "392       483  fotografia manaus food me kitchen ego myself c...\n",
      "393       484  france   infrared provence rhnealpes aplusphot...\n",
      "394       486  stx stcroix stx stcroix stx stcroix stx stcroi...\n",
      "395       487      svalbard val semester resa djur valar sillval\n",
      "396       488  lauren halloween amelia kaduce halloween ameli...\n",
      "397       490  desktop wallpaper 800x600 psp background wides...\n",
      "398       492  alaska seaice beringsea noaashipmcarthurii ala...\n",
      "399       493  africa travel work uganda kampala africa bird ...\n",
      "400       494  travel flores viagens azores aores caminhadas ...\n",
      "401       495  october property greece friday neighbours zaky...\n",
      "402       496  tower  airshow flyin overberg aviocar dopplerr...\n",
      "403       497  trees lake ontario canada water boats berfordl...\n",
      "404       498      ocean sea vacation ship iphone 0810 iphone365\n",
      "405       499  mexico graphicdesign mexicocity humor folklore...\n",
      "\n",
      "[406 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# I will save all the tags for 500 clusters into 500 files\n",
    "# dataset = load_files(container_path = 'data/txt-combined/',shuffle=False)\n",
    "\n",
    "df = pd.read_csv(\"500clusters_with_tags.csv\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py:211: UserWarning: cannot use tree with sparse input: using brute force\n",
      "  warnings.warn(\"cannot use tree with sparse input: \"\n"
     ]
    }
   ],
   "source": [
    "# so we have 500 clusters with correspondanse values of latitude and longitude , and concatenated texts,\n",
    "# we compare this clusters with the texts of the test sets \n",
    "train_data = pd.read_csv('500clusters_with_tags.csv')\n",
    "train_data_ = train_data.loc[:,'photoTags']\n",
    "_vect = TfidfVectorizer()\n",
    "train_c_matrix = _vect.fit_transform(train_data_)\n",
    "vocab = _vect.vocabulary_\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors = 1, algorithm='ball_tree')\n",
    "nbrs.fit(train_c_matrix)\n",
    "\n",
    "test_data = pd.read_csv('test2_all.csv')\n",
    "# delete all data with empty tags from test data\n",
    "#test_data.dropna(subset=['photoTags'], inplace=True)\n",
    "test_data['photoTags'].replace( np.nan,'', inplace=True)\n",
    "test_data_ = test_data.loc[:,'photoTags']\n",
    "new_vect = TfidfVectorizer(vocabulary = vocab)\n",
    "test_c_matrix = new_vect.fit_transform(test_data_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-16 08:38:47.562685\n",
      "0\n",
      "2018-05-16 08:38:47.912677\n",
      "100\n",
      "2018-05-16 08:38:58.413041\n",
      "200\n",
      "2018-05-16 08:39:08.933384\n",
      "300\n",
      "2018-05-16 08:39:19.483710\n",
      "400\n",
      "2018-05-16 08:39:29.994054\n",
      "500\n",
      "2018-05-16 08:39:40.474396\n",
      "600\n",
      "2018-05-16 08:39:50.934760\n",
      "700\n",
      "2018-05-16 08:40:01.415103\n",
      "800\n",
      "2018-05-16 08:40:11.816595\n",
      "900\n",
      "2018-05-16 08:40:22.196963\n",
      "1000\n",
      "2018-05-16 08:40:32.767373\n",
      "1100\n",
      "2018-05-16 08:40:43.297762\n",
      "1200\n",
      "2018-05-16 08:40:53.788190\n",
      "1300\n",
      "2018-05-16 08:41:04.398600\n",
      "1400\n",
      "2018-05-16 08:41:14.999018\n",
      "1500\n",
      "2018-05-16 08:41:25.709426\n",
      "1600\n",
      "2018-05-16 08:41:36.964784\n",
      "1700\n",
      "2018-05-16 08:41:47.585196\n",
      "1800\n",
      "2018-05-16 08:41:58.125625\n",
      "1900\n",
      "2018-05-16 08:42:08.626030\n",
      "2000\n",
      "2018-05-16 08:42:19.046418\n",
      "2100\n",
      "2018-05-16 08:42:29.436822\n",
      "2200\n",
      "2018-05-16 08:42:39.837247\n",
      "2300\n",
      "2018-05-16 08:42:50.247651\n",
      "2400\n",
      "2018-05-16 08:43:00.648035\n",
      "2500\n",
      "2018-05-16 08:43:11.088461\n",
      "2600\n",
      "2018-05-16 08:43:21.538847\n",
      "2700\n",
      "2018-05-16 08:43:31.993924\n",
      "2800\n",
      "2018-05-16 08:43:42.454329\n",
      "2900\n",
      "2018-05-16 08:43:52.884754\n",
      "3000\n",
      "2018-05-16 08:44:03.325139\n",
      "3100\n",
      "2018-05-16 08:44:13.765546\n",
      "3200\n",
      "2018-05-16 08:44:24.255971\n",
      "3300\n",
      "2018-05-16 08:44:34.876366\n",
      "3400\n",
      "2018-05-16 08:44:45.536804\n",
      "3500\n",
      "2018-05-16 08:44:56.537208\n",
      "3600\n",
      "2018-05-16 08:45:07.397629\n",
      "3700\n",
      "2018-05-16 08:45:18.338056\n",
      "3800\n",
      "2018-05-16 08:45:29.306896\n",
      "3900\n",
      "2018-05-16 08:45:40.201763\n",
      "4000\n",
      "2018-05-16 08:45:51.112185\n",
      "4100\n",
      "2018-05-16 08:46:02.002629\n",
      "4200\n",
      "2018-05-16 08:46:12.953035\n",
      "4300\n",
      "2018-05-16 08:46:23.863462\n",
      "4400\n",
      "2018-05-16 08:46:34.753882\n",
      "4500\n",
      "2018-05-16 08:46:45.644304\n",
      "4600\n",
      "2018-05-16 08:46:56.554748\n",
      "4700\n",
      "2018-05-16 08:47:07.565178\n",
      "4800\n",
      "2018-05-16 08:47:18.689131\n",
      "4900\n",
      "2018-05-16 08:47:29.817796\n",
      "5000\n",
      "2018-05-16 08:47:40.798243\n",
      "5100\n",
      "2018-05-16 08:47:52.288670\n",
      "5200\n",
      "2018-05-16 08:48:03.849117\n",
      "5300\n",
      "2018-05-16 08:48:14.899551\n",
      "5400\n",
      "2018-05-16 08:48:26.009980\n",
      "5500\n",
      "2018-05-16 08:48:37.190414\n",
      "5600\n",
      "2018-05-16 08:48:48.450850\n",
      "5700\n",
      "2018-05-16 08:48:59.501281\n",
      "5800\n",
      "2018-05-16 08:49:10.701715\n",
      "5900\n",
      "2018-05-16 08:49:21.892170\n",
      "6000\n",
      "2018-05-16 08:49:33.132586\n",
      "6100\n",
      "2018-05-16 08:49:44.253019\n",
      "6200\n",
      "2018-05-16 08:49:55.353451\n",
      "6300\n",
      "2018-05-16 08:50:06.563889\n",
      "6400\n",
      "2018-05-16 08:50:17.674317\n",
      "6500\n",
      "2018-05-16 08:50:28.524760\n",
      "6600\n",
      "2018-05-16 08:50:39.515187\n",
      "6700\n",
      "2018-05-16 08:50:50.395588\n",
      "6800\n",
      "2018-05-16 08:51:01.316014\n",
      "6900\n",
      "2018-05-16 08:51:12.256438\n",
      "7000\n",
      "2018-05-16 08:51:23.206887\n",
      "7100\n",
      "2018-05-16 08:51:34.287294\n",
      "7200\n",
      "2018-05-16 08:51:45.177718\n",
      "7300\n",
      "2018-05-16 08:51:56.218146\n",
      "7400\n",
      "2018-05-16 08:52:07.258575\n",
      "7500\n",
      "2018-05-16 08:52:18.279003\n",
      "7600\n",
      "2018-05-16 08:52:29.189449\n",
      "7700\n",
      "2018-05-16 08:52:40.109851\n",
      "7800\n",
      "2018-05-16 08:52:51.080300\n",
      "7900\n",
      "2018-05-16 08:53:01.980701\n",
      "8000\n",
      "2018-05-16 08:53:12.951127\n",
      "8100\n",
      "2018-05-16 08:53:23.881552\n",
      "8200\n",
      "2018-05-16 08:53:35.081987\n",
      "8300\n",
      "2018-05-16 08:53:46.267146\n",
      "8400\n",
      "2018-05-16 08:53:57.348275\n",
      "8500\n",
      "2018-05-16 08:54:07.956926\n",
      "8600\n",
      "2018-05-16 08:54:18.567341\n",
      "8700\n",
      "2018-05-16 08:54:29.117745\n",
      "8800\n",
      "2018-05-16 08:54:39.678138\n",
      "8900\n",
      "2018-05-16 08:54:50.698587\n",
      "9000\n",
      "2018-05-16 08:55:01.628990\n",
      "9100\n",
      "2018-05-16 08:55:12.549415\n",
      "9200\n",
      "2018-05-16 08:55:23.479839\n",
      "9300\n",
      "2018-05-16 08:55:34.480775\n",
      "9400\n",
      "2018-05-16 08:55:45.361175\n",
      "9500\n",
      "2018-05-16 08:55:56.341604\n",
      "9600\n",
      "2018-05-16 08:56:07.302028\n",
      "9700\n",
      "2018-05-16 08:56:18.302460\n",
      "9800\n",
      "2018-05-16 08:56:29.302904\n",
      "9900\n",
      "2018-05-16 08:56:40.333311\n",
      "10000\n",
      "2018-05-16 08:56:51.313739\n",
      "10100\n",
      "2018-05-16 08:57:02.274165\n",
      "10200\n",
      "2018-05-16 08:57:13.254591\n",
      "10300\n",
      "2018-05-16 08:57:24.145013\n",
      "10400\n",
      "2018-05-16 08:57:35.035436\n",
      "10500\n",
      "2018-05-16 08:57:46.045887\n",
      "10600\n",
      "2018-05-16 08:57:56.986311\n",
      "10700\n",
      "2018-05-16 08:58:07.996717\n",
      "10800\n",
      "2018-05-16 08:58:18.957143\n",
      "10900\n",
      "2018-05-16 08:58:30.042486\n",
      "11000\n",
      "2018-05-16 08:58:41.232923\n",
      "11100\n",
      "2018-05-16 08:58:52.313377\n",
      "11200\n",
      "2018-05-16 08:59:03.323779\n",
      "11300\n",
      "2018-05-16 08:59:14.334207\n",
      "11400\n",
      "2018-05-16 08:59:25.294634\n",
      "11500\n",
      "2018-05-16 08:59:36.405067\n",
      "11600\n",
      "2018-05-16 08:59:47.335489\n",
      "11700\n",
      "2018-05-16 08:59:58.325916\n",
      "11800\n",
      "2018-05-16 09:00:09.348718\n",
      "11900\n",
      "2018-05-16 09:00:20.319149\n",
      "12000\n",
      "2018-05-16 09:00:31.399574\n",
      "12100\n",
      "2018-05-16 09:00:42.400003\n",
      "12200\n",
      "2018-05-16 09:00:53.350428\n",
      "12300\n",
      "2018-05-16 09:01:04.500884\n",
      "12400\n",
      "2018-05-16 09:01:15.591293\n",
      "12500\n",
      "2018-05-16 09:01:26.551718\n",
      "12600\n",
      "2018-05-16 09:01:37.562149\n",
      "12700\n",
      "2018-05-16 09:01:48.502573\n",
      "12800\n",
      "2018-05-16 09:01:59.543004\n",
      "12900\n",
      "2018-05-16 09:02:10.542622\n",
      "13000\n",
      "2018-05-16 09:02:21.543048\n",
      "13100\n",
      "2018-05-16 09:02:32.583500\n",
      "13200\n",
      "2018-05-16 09:02:43.843914\n",
      "13300\n",
      "2018-05-16 09:02:55.074351\n",
      "13400\n",
      "2018-05-16 09:03:06.034778\n",
      "13500\n",
      "2018-05-16 09:03:17.165209\n",
      "13600\n",
      "2018-05-16 09:03:28.234246\n",
      "13700\n",
      "2018-05-16 09:03:39.264677\n",
      "13800\n",
      "2018-05-16 09:03:50.375106\n",
      "13900\n",
      "2018-05-16 09:04:01.435534\n",
      "14000\n",
      "2018-05-16 09:04:12.405982\n",
      "14100\n",
      "2018-05-16 09:04:23.346388\n",
      "14200\n",
      "2018-05-16 09:04:34.386815\n",
      "14300\n",
      "2018-05-16 09:04:45.387247\n",
      "14400\n",
      "2018-05-16 09:04:56.417672\n",
      "14500\n",
      "2018-05-16 09:05:07.408122\n",
      "14600\n",
      "2018-05-16 09:05:18.418526\n",
      "14700\n",
      "2018-05-16 09:05:29.358973\n",
      "14800\n",
      "2018-05-16 09:05:40.409384\n",
      "14900\n",
      "2018-05-16 09:05:52.043277\n",
      "15000\n",
      "2018-05-16 09:06:03.343715\n",
      "15100\n",
      "2018-05-16 09:06:14.424146\n",
      "15200\n",
      "2018-05-16 09:06:25.603840\n",
      "15300\n",
      "2018-05-16 09:06:36.716848\n",
      "15400\n",
      "2018-05-16 09:06:48.087861\n",
      "15500\n",
      "2018-05-16 09:06:59.687949\n",
      "15600\n",
      "2018-05-16 09:07:10.830676\n",
      "15700\n",
      "2018-05-16 09:07:22.037031\n",
      "15800\n",
      "2018-05-16 09:07:33.160145\n",
      "15900\n",
      "2018-05-16 09:07:44.325316\n",
      "16000\n",
      "2018-05-16 09:07:55.463956\n",
      "16100\n",
      "2018-05-16 09:08:06.607032\n",
      "16200\n",
      "2018-05-16 09:08:17.712616\n",
      "16300\n",
      "2018-05-16 09:08:28.845266\n",
      "16400\n",
      "2018-05-16 09:08:39.979434\n",
      "16500\n",
      "2018-05-16 09:08:51.084331\n",
      "16600\n",
      "2018-05-16 09:09:02.335713\n",
      "16700\n",
      "2018-05-16 09:09:13.493385\n",
      "16800\n",
      "2018-05-16 09:09:24.640538\n",
      "16900\n",
      "2018-05-16 09:09:35.813528\n",
      "17000\n",
      "2018-05-16 09:09:47.044526\n",
      "17100\n",
      "2018-05-16 09:09:58.290015\n",
      "17200\n",
      "2018-05-16 09:10:09.490272\n",
      "17300\n",
      "2018-05-16 09:10:20.662053\n",
      "17400\n",
      "2018-05-16 09:10:31.869177\n",
      "17500\n",
      "2018-05-16 09:10:43.064628\n",
      "17600\n",
      "2018-05-16 09:10:54.044577\n",
      "17700\n",
      "2018-05-16 09:11:05.142726\n",
      "17800\n",
      "2018-05-16 09:11:16.228841\n",
      "17900\n",
      "2018-05-16 09:11:27.182131\n",
      "18000\n",
      "2018-05-16 09:11:38.173162\n",
      "18100\n",
      "2018-05-16 09:11:49.136392\n",
      "18200\n",
      "2018-05-16 09:12:00.192940\n",
      "18300\n",
      "2018-05-16 09:12:11.221528\n",
      "18400\n",
      "2018-05-16 09:12:22.232675\n",
      "18500\n",
      "2018-05-16 09:12:33.223821\n",
      "18600\n",
      "2018-05-16 09:12:44.216272\n",
      "18700\n",
      "2018-05-16 09:12:55.212531\n",
      "18800\n",
      "2018-05-16 09:13:06.182897\n",
      "18900\n",
      "2018-05-16 09:13:17.169938\n",
      "19000\n",
      "2018-05-16 09:13:28.213679\n",
      "19100\n",
      "2018-05-16 09:13:39.248428\n",
      "19200\n",
      "2018-05-16 09:13:50.284178\n",
      "19300\n",
      "2018-05-16 09:14:01.296534\n",
      "19400\n",
      "2018-05-16 09:14:12.296929\n",
      "19500\n",
      "2018-05-16 09:14:23.262371\n",
      "19600\n",
      "2018-05-16 09:14:34.217114\n",
      "19700\n",
      "2018-05-16 09:14:45.207869\n",
      "19800\n",
      "2018-05-16 09:14:56.169874\n",
      "19900\n",
      "2018-05-16 09:15:07.198604\n",
      "20000\n",
      "2018-05-16 09:15:18.221274\n",
      "20100\n",
      "2018-05-16 09:15:29.213014\n",
      "20200\n",
      "2018-05-16 09:15:40.210986\n",
      "20300\n",
      "2018-05-16 09:15:51.192902\n",
      "20400\n",
      "2018-05-16 09:16:02.164740\n",
      "20500\n",
      "2018-05-16 09:16:13.168587\n",
      "20600\n",
      "2018-05-16 09:16:24.150812\n",
      "20700\n",
      "2018-05-16 09:16:35.123749\n",
      "20800\n",
      "2018-05-16 09:16:46.143336\n",
      "20900\n",
      "2018-05-16 09:16:57.131742\n",
      "21000\n",
      "2018-05-16 09:17:08.118399\n",
      "21100\n",
      "2018-05-16 09:17:19.153751\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('500_clusters.csv')\n",
    "import datetime\n",
    "predicted_lt = []\n",
    "predicted_lng = []\n",
    "#indexes = []\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "for i in range(0,len(test_data_)):\n",
    "    #distances, indices = nbrs.kneighbors(svd_test_matrix[[i]])\n",
    "    \n",
    "    distances, indices = nbrs.kneighbors(test_c_matrix[i])\n",
    "    #distances, indices = tree.query([svd_test_matrix[i]], k=1)\n",
    "    ind = indices[0][0]\n",
    "    #indexes.append(indices[0][0])\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "        print(datetime.datetime.now())            \n",
    "    #print('ind',ind)\n",
    "    #print(distances)\n",
    "    \n",
    "    cl = train_data.loc[ind,'clusters']\n",
    "    lt = df.loc[cl,'latitude']\n",
    "    predicted_lt.append(lt)\n",
    "    lng = df.loc[cl,'longitude']\n",
    "    predicted_lng.append(lng)\n",
    "    \n",
    "\n",
    "    #df_lt = pd.DataFrame([lt], columns=['lt'])\n",
    "    #df_ln = pd.DataFrame([lng], columns=['ln'])\n",
    "    \n",
    "    #data_lt = data_lt.append(df_lt)\n",
    "    #data_ln = data_ln.append(df_ln)\n",
    "    #print('test:', test_data.loc[i,'photoTags'])\n",
    "    #print('train:', train_data.loc[ind,'photoTags'])\n",
    "    #print('-----------------------------')\n",
    "    \n",
    "\n",
    "test_data['lt_predicted'] = predicted_lt\n",
    "test_data['lng_predicted'] = predicted_lng\n",
    "\n",
    "test_data.to_csv(\"predicted_2_clusters.csv\", encoding=\"utf-8\")\n",
    "\n",
    "data_predicted = pd.read_csv('predicted_2_clusters.csv')\n",
    "\n",
    "# let's calculate the error and save it in a new column \n",
    "lt_error = (data_predicted['lt_predicted'] - data_predicted['latitude'])**(2)\n",
    "ln_error = (data_predicted['lng_predicted'] - data_predicted['longitude'])**(2)\n",
    "er = (lt_error + ln_error)**(0.5)\n",
    "data_predicted['error'] = er\n",
    "\n",
    "# we will see radius of 0.001 km, 0.01 km, 1 km, 10 km , 100 km , 1000 km, 10000 km, 40000 km \n",
    "\n",
    "data_predicted.to_csv(\"predicted_2_clusters.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1e-05, 0.0001, 0.01, 0.1, 1, 10, 100, 400]\n",
      "[0, 0, 5, 433, 5131, 10205, 16107, 21200]\n",
      "[0.0, 0.0, 0.00023584905660377359, 0.020424528301886794, 0.24202830188679245, 0.4813679245283019, 0.75976415094339622, 1.0]\n"
     ]
    }
   ],
   "source": [
    "data_predicted = pd.read_csv('predicted_2_clusters.csv')\n",
    "\n",
    "# let's calculate the error and save it in a new column \n",
    "lt_error = (data_predicted['lt_predicted'] - data_predicted['latitude'])**(2)\n",
    "ln_error = (data_predicted['lng_predicted'] - data_predicted['longitude'])**(2)\n",
    "er = (lt_error + ln_error)**(0.5)\n",
    "data_predicted['error'] = er\n",
    "\n",
    "# we will see radius of 0.001 km, 0.01 km, 1 km, 10 km , 100 km , 1000 km, 10000 km, 40000 km \n",
    "\n",
    "data_predicted.to_csv(\"predicted_2_clusters_errors.csv\", encoding=\"utf-8\")\n",
    "radius = [0.00001, 0.0001, 0.01, 0.1, 1, 10, 100, 400]\n",
    "\n",
    "# 1 latitude = 111 km , so we will round it to 100 km\n",
    "answers = []\n",
    "for el in radius: \n",
    "    answers.append(data_predicted['error'][data_predicted['error']<el].count())\n",
    "accuracy = []\n",
    "amount = len(data_predicted)\n",
    "print(radius)\n",
    "print(answers)\n",
    "for i in range(0,8):\n",
    "    accuracy.append(answers[i]/amount)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
