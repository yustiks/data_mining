{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take tags only from test5 database\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63463\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test3_all.csv')\n",
    "\n",
    "# delete all data with empty tags from test data\n",
    "test_data.dropna(subset=['photoTags'], inplace=True)\n",
    "\n",
    "#take tags from the column 'photoTags'\n",
    "tags = test_data.loc[:,'photoTags']\n",
    "\n",
    "# join them to list\n",
    "text = ' '.join(test_data['photoTags'])\n",
    "text_list = text.split(' ')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(tags)\n",
    "\n",
    "\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "\n",
    "\n",
    "# save the corpus into the file\n",
    "import csv\n",
    "\n",
    "dt = vectorizer.vocabulary_\n",
    "\n",
    "\n",
    "d = pd.DataFrame.from_dict(dt, orient='index', dtype=None)\n",
    "\n",
    "d.to_csv(\"corpus1.csv\", encoding=\"utf-8\", header=None)\n",
    "\n",
    "#LOAD DATA\n",
    "crp = pd.read_csv(\"corpus1.csv\", header=None)\n",
    "crp.to_csv(\"corpus1.csv\", encoding=\"utf-8\", header=['words','id'], index=None)\n",
    "\n",
    "#LOAD DATA\n",
    "crp = pd.read_csv(\"corpus1.csv\")\n",
    "\n",
    "#CLEAN DATA\n",
    "crp['words'] = crp['words'].str.replace(\"\\d+\", '')\n",
    "crp['words'] = crp['words'].str.replace(\"sec\", '')\n",
    "crp['words'] = crp['words'].str.replace(\"nabkv\", '')\n",
    "# delete values less then 3\n",
    "crp =crp[crp['words'].str.len() > 3]\n",
    "# delete geolat geolon\n",
    "crp['words'] = crp['words'].str.replace(\"geolat\", '')\n",
    "crp['words'] = crp['words'].str.replace(\"geolon\", '')\n",
    "crp['words'] = crp['words'].str.replace(\"geotagged\", '')\n",
    "\n",
    "#train['words'] = train['words'].str.replace(\"^[0-9]*$\", '')\n",
    "#s = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", s)\n",
    "crp['words'].replace('', np.nan, inplace=True)\n",
    "crp = crp.dropna(subset=['words'])\n",
    "\n",
    "crp = crp[crp['words'].str.len() > 3]\n",
    "\n",
    "# save data\n",
    "crp.to_csv(\"corpus.csv\", encoding=\"utf-8\",index=None)\n",
    "\n",
    "# load data again \n",
    "crp = pd.read_csv('corpus.csv')\n",
    "#take tags from the column 'photoTags'\n",
    "tags = crp.loc[:,'words']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py:211: UserWarning: cannot use tree with sparse input: using brute force\n",
      "  warnings.warn(\"cannot use tree with sparse input: \"\n"
     ]
    }
   ],
   "source": [
    "# so we have features \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tf_matrix = tfidf.fit_transform(tags)\n",
    "\n",
    "vocab = tfidf.vocabulary_\n",
    "\n",
    "# we have sparse matrix for 3 mln pictures \n",
    "# create KNN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "train_data = pd.read_csv('dm_clean1.csv')\n",
    "train_data_ = train_data.loc[:,'photoTags']\n",
    "_vect = TfidfVectorizer(vocabulary = vocab)\n",
    "train_c_matrix = _vect.fit_transform(train_data_)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors = 1, algorithm='ball_tree')\n",
    "nbrs.fit(train_c_matrix)\n",
    "\n",
    "test_data = pd.read_csv('test3_all.csv')\n",
    "# delete all data with empty tags from test data\n",
    "#test_data.dropna(subset=['photoTags'], inplace=True)\n",
    "test_data['photoTags'].replace( np.nan,'', inplace=True)\n",
    "test_data_ = test_data.loc[:,'photoTags']\n",
    "new_vect = TfidfVectorizer(vocabulary = vocab)\n",
    "test_c_matrix = new_vect.fit_transform(test_data_)\n",
    "\n",
    "predicted_lt = []\n",
    "predicted_lng = []\n",
    "\n",
    "for i in range(0,len(test_data_)):\n",
    "    distances, indices = nbrs.kneighbors(test_c_matrix[i])\n",
    "\n",
    "    ind = indices[0][0]\n",
    "\n",
    "    #print('ind',ind)\n",
    "    #print(distances)\n",
    "\n",
    "    lt =train_data.loc[ind,'latitude']\n",
    "    predicted_lt.append(lt)\n",
    "    lng = train_data.loc[ind,'longitude']\n",
    "    predicted_lng.append(lng)\n",
    "    \n",
    "    #df_lt = pd.DataFrame([lt], columns=['lt'])\n",
    "    #df_ln = pd.DataFrame([lng], columns=['ln'])\n",
    "    \n",
    "    #data_lt = data_lt.append(df_lt)\n",
    "    #data_ln = data_ln.append(df_ln)\n",
    "    #print('test:', test_data.loc[i,'photoTags'])\n",
    "    #print('train:', train_data.loc[ind,'photoTags'])\n",
    "    #print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data['lt_predicted'] = predicted_lt\n",
    "test_data['lng_predicted'] = predicted_lng\n",
    "\n",
    "test_data.to_csv(\"predicted_2.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_predicted = pd.read_csv('predicted_2.csv')\n",
    "\n",
    "# let's calculate the error and save it in a new column \n",
    "lt_error = (data_predicted['lt_predicted'] - data_predicted['latitude'])**(2)\n",
    "ln_error = (data_predicted['lng_predicted'] - data_predicted['longitude'])**(2)\n",
    "er = (lt_error + ln_error)**(0.5)\n",
    "data_predicted['error'] = er\n",
    "\n",
    "# we will see radius of 0.001 km, 0.01 km, 1 km, 10 km , 100 km , 1000 km, 10000 km, 40000 km \n",
    "\n",
    "data_predicted.to_csv(\"predicted_2_error.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
